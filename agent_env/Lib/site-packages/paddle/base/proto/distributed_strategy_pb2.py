# -*- coding: utf-8 -*-
# Generated by the protocol buffer compiler.  DO NOT EDIT!
# source: paddle/fluid/framework/distributed_strategy.proto
"""Generated protocol buffer code."""
from google.protobuf.internal import builder as _builder
from google.protobuf import descriptor as _descriptor
from google.protobuf import descriptor_pool as _descriptor_pool
from google.protobuf import symbol_database as _symbol_database
# @@protoc_insertion_point(imports)

_sym_db = _symbol_database.Default()




DESCRIPTOR = _descriptor_pool.Default().AddSerializedFile(b'\n1paddle/fluid/framework/distributed_strategy.proto\x12\x0cpaddle.fleet\"W\n\x11RefinedOpsPattern\x12\x10\n\x08main_ops\x18\x01 \x03(\t\x12\x0e\n\x03num\x18\x02 \x01(\x05:\x01\x30\x12\x0f\n\x07pre_ops\x18\x03 \x03(\t\x12\x0f\n\x07suf_ops\x18\x04 \x03(\t\"\xbc\x01\n\x0fRecomputeConfig\x12\x13\n\x0b\x63heckpoints\x18\x01 \x03(\t\x12\x1d\n\x0e\x65nable_offload\x18\x02 \x01(\x08:\x05\x66\x61lse\x12\x18\n\x10\x63heckpoint_shape\x18\x03 \x03(\x05\x12\x1c\n\renable_tuning\x18\x04 \x01(\x08:\x05\x66\x61lse\x12=\n\x14refined_ops_patterns\x18\x05 \x03(\x0b\x32\x1f.paddle.fleet.RefinedOpsPattern\"\x82\x04\n\x0eShardingConfig\x12\x37\n\x19sharding_segment_strategy\x18\x01 \x01(\t:\x14segment_broadcast_MB\x12 \n\x14segment_broadcast_MB\x18\x02 \x01(\x02:\x02\x33\x32\x12\x17\n\x0fsegment_anchors\x18\x03 \x03(\t\x12\x1a\n\x0fsharding_degree\x18\x04 \x01(\x05:\x01\x38\x12\x14\n\tmp_degree\x18\x05 \x01(\x05:\x01\x31\x12\x14\n\tdp_degree\x18\x06 \x01(\x05:\x01\x31\x12\x18\n\thybrid_dp\x18\x07 \x01(\x08:\x05\x66\x61lse\x12\"\n\x17gradient_merge_acc_step\x18\x08 \x01(\x05:\x01\x31\x12\x1f\n\x10optimize_offload\x18\t \x01(\x08:\x05\x66\x61lse\x12\'\n\x18pp_allreduce_in_optimize\x18\n \x01(\x08:\x05\x66\x61lse\x12\x14\n\tpp_degree\x18\x0b \x01(\x05:\x01\x31\x12\x1c\n\roptimize_cast\x18\x0c \x01(\x08:\x05\x66\x61lse\x12(\n\x19_dp_as_optimizer_sharding\x18\r \x01(\x08:\x05\x66\x61lse\x12\x10\n\x05stage\x18\x0e \x01(\x05:\x01\x31\x12\x1c\n\renable_tuning\x18\x0f \x01(\x08:\x05\x66\x61lse\x12\x1e\n\x0fuse_calc_stream\x18\x10 \x01(\x08:\x05\x66\x61lse\"\xdc\x02\n\x08MpConfig\x12\x18\n\nsync_param\x18\x01 \x01(\x08:\x04true\x12\x18\n\tsync_grad\x18\x02 \x01(\x08:\x05\x66\x61lse\x12\x1a\n\x0bsync_moment\x18\x03 \x01(\x08:\x05\x66\x61lse\x12\x1c\n\tsync_mode\x18\x04 \x01(\t:\tbroadcast\x12!\n\x12mp_async_allreduce\x18\x05 \x01(\x08:\x05\x66\x61lse\x12!\n\x12mp_skip_c_identity\x18\x06 \x01(\x08:\x05\x66\x61lse\x12-\n\x1emp_fused_linear_param_grad_add\x18\x07 \x01(\x08:\x05\x66\x61lse\x12!\n\x13need_broadcast_data\x18\x08 \x01(\x08:\x04true\x12\"\n\x13recompute_allgather\x18\t \x01(\x08:\x05\x66\x61lse\x12&\n\x17sp_async_reduce_scatter\x18\n \x01(\x08:\x05\x66\x61lse\"\xf9\x03\n\x08PpConfig\x12\x1e\n\x0f\x64p_comm_overlap\x18\x01 \x01(\x08:\x05\x66\x61lse\x12\x1f\n\x10\x64\x65lay_scale_loss\x18\x02 \x01(\x08:\x05\x66\x61lse\x12\x1b\n\x0c\x65nable_timer\x18\x03 \x01(\x08:\x05\x66\x61lse\x12$\n\x15sharding_comm_overlap\x18\x04 \x01(\x08:\x05\x66\x61lse\x12\x18\n\tprofiling\x18\x05 \x01(\x08:\x05\x66\x61lse\x12 \n\x11release_gradients\x18\x06 \x01(\x08:\x05\x66\x61lse\x12\x1f\n\x10overlap_p2p_comm\x18\x07 \x01(\x08:\x05\x66\x61lse\x12%\n\x16\x63lear_every_step_cache\x18\x08 \x01(\x08:\x05\x66\x61lse\x12 \n\x12use_batch_p2p_comm\x18\t \x01(\x08:\x04true\x12(\n\x19\x62\x65st_unbalanced_scheduler\x18\n \x01(\x08:\x05\x66\x61lse\x12#\n\x14\x65nable_offload_queue\x18\x0b \x01(\x08:\x05\x66\x61lse\x12#\n\x14\x65nable_dynamic_shape\x18\x0c \x01(\x08:\x05\x66\x61lse\x12\x1c\n\ruse_dualpipev\x18\r \x01(\x08:\x05\x66\x61lse\x12\x31\n\"forward_backward_overlap_scheduler\x18\x0e \x01(\x08:\x05\x66\x61lse\"\xdb\x02\n\x15\x44ygraphShardingConfig\x12\x1c\n\rtensor_fusion\x18\x01 \x01(\x08:\x05\x66\x61lse\x12\x1b\n\x10\x61\x63\x63umulate_steps\x18\x02 \x01(\x05:\x01\x31\x12\x1b\n\x0c\x63omm_overlap\x18\x03 \x01(\x08:\x05\x66\x61lse\x12\x1a\n\x0bsplit_param\x18\x04 \x01(\x08:\x05\x66\x61lse\x12\x1c\n\x0e\x66use_optimizer\x18\x05 \x01(\x08:\x04true\x12\x1c\n\x0euse_reduce_avg\x18\x06 \x01(\x08:\x04true\x12 \n\x13\x63omm_buffer_size_MB\x18\x07 \x01(\x05:\x03\x32\x35\x36\x12 \n\x11release_gradients\x18\x08 \x01(\x08:\x05\x66\x61lse\x12!\n\x12\x66ree_grads_in_comm\x18\t \x01(\x08:\x05\x66\x61lse\x12+\n\x1c\x65nable_fuse_optimizer_states\x18\n \x01(\x08:\x05\x66\x61lse\"\x98\x03\n\x0cHybridConfig\x12\x15\n\tdp_degree\x18\x01 \x01(\x05:\x02-1\x12\x14\n\tmp_degree\x18\x02 \x01(\x05:\x01\x31\x12\x14\n\tpp_degree\x18\x03 \x01(\x05:\x01\x31\x12\x1a\n\x0fsharding_degree\x18\x04 \x01(\x05:\x01\x31\x12\x15\n\nsep_degree\x18\x05 \x01(\x05:\x01\x31\x12*\n\nmp_configs\x18\x06 \x01(\x0b\x32\x16.paddle.fleet.MpConfig\x12*\n\npp_configs\x18\x07 \x01(\x0b\x32\x16.paddle.fleet.PpConfig\x12=\n\x10sharding_configs\x18\x08 \x01(\x0b\x32#.paddle.fleet.DygraphShardingConfig\x12%\n\x16\x65nable_optimizer_timer\x18\t \x01(\x08:\x05\x66\x61lse\x12\x1e\n\x0fsplit_norm_comm\x18\n \x01(\x08:\x05\x66\x61lse\x12\x14\n\tep_degree\x18\x0b \x01(\x05:\x01\x31\x12\x1e\n\x13moe_sharding_degree\x18\x0c \x01(\x05:\x01\x31\"\x9d\x03\n\tAMPConfig\x12 \n\x11init_loss_scaling\x18\x01 \x01(\x02:\x05\x33\x32\x37\x36\x38\x12 \n\x12incr_every_n_steps\x18\x02 \x01(\x05:\x04\x31\x30\x30\x30\x12\"\n\x17\x64\x65\x63r_every_n_nan_or_inf\x18\x03 \x01(\x05:\x01\x32\x12\x15\n\nincr_ratio\x18\x04 \x01(\x02:\x01\x32\x12\x17\n\ndecr_ratio\x18\x05 \x01(\x02:\x03\x30.8\x12&\n\x18use_dynamic_loss_scaling\x18\x06 \x01(\x08:\x04true\x12\x19\n\x11\x63ustom_white_list\x18\x07 \x03(\t\x12\x19\n\x11\x63ustom_black_list\x18\x08 \x03(\t\x12\x1d\n\x15\x63ustom_black_varnames\x18\t \x03(\t\x12\x1c\n\ruse_pure_fp16\x18\n \x01(\x08:\x05\x66\x61lse\x12\x1c\n\x0euse_fp16_guard\x18\x0b \x01(\x08:\x04true\x12!\n\x12use_optimizer_fp16\x18\x0c \x01(\x08:\x05\x66\x61lse\x12\x1c\n\ruse_pure_bf16\x18\r \x01(\x08:\x05\x66\x61lse\";\n\x0eLocalSGDConfig\x12\x12\n\x07k_steps\x18\x01 \x01(\x05:\x01\x31\x12\x15\n\nbegin_step\x18\x02 \x01(\x05:\x01\x31\"H\n\x16\x41\x64\x61ptiveLocalSGDConfig\x12\x17\n\x0cinit_k_steps\x18\x01 \x01(\x05:\x01\x31\x12\x15\n\nbegin_step\x18\x02 \x01(\x05:\x01\x31\"<\n\x13GradientMergeConfig\x12\x12\n\x07k_steps\x18\x01 \x01(\x05:\x01\x31\x12\x11\n\x03\x61vg\x18\x02 \x01(\x08:\x04true\"S\n\tDGCConfig\x12\x1c\n\x11rampup_begin_step\x18\x01 \x01(\x05:\x01\x30\x12\x16\n\x0brampup_step\x18\x02 \x01(\x05:\x01\x31\x12\x10\n\x08sparsity\x18\x03 \x03(\x02\"\x81\x01\n\nLarsConfig\x12\x19\n\nlars_coeff\x18\x01 \x01(\x02:\x05\x30.001\x12!\n\x11lars_weight_decay\x18\x02 \x01(\x02:\x06\x30.0005\x12\x12\n\x07\x65psilon\x18\x03 \x01(\x02:\x01\x30\x12!\n\x19\x65xclude_from_weight_decay\x18\x04 \x03(\t\"P\n\nLambConfig\x12\x1f\n\x11lamb_weight_decay\x18\x01 \x01(\x02:\x04\x30.01\x12!\n\x19\x65xclude_from_weight_decay\x18\x02 \x03(\t\"\xb3\x05\n\rBuildStrategy\x12\'\n\x18\x66use_elewise_add_act_ops\x18\x02 \x01(\x08:\x05\x66\x61lse\x12\x1e\n\x0f\x66use_bn_act_ops\x18\x03 \x01(\x08:\x05\x66\x61lse\x12\'\n\x18\x66use_relu_depthwise_conv\x18\x04 \x01(\x08:\x05\x66\x61lse\x12!\n\x12\x66use_broadcast_ops\x18\x05 \x01(\x08:\x05\x66\x61lse\x12%\n\x16\x66use_all_optimizer_ops\x18\x06 \x01(\x08:\x05\x66\x61lse\x12\x1d\n\x0e\x65nable_inplace\x18\x07 \x01(\x08:\x05\x66\x61lse\x12/\n!enable_backward_optimizer_op_deps\x18\x08 \x01(\x08:\x04true\x12$\n\x15\x63\x61\x63he_runtime_context\x18\t \x01(\x08:\x05\x66\x61lse\x12!\n\x13\x66use_bn_add_act_ops\x18\n \x01(\x08:\x04true\x12!\n\x12\x65nable_auto_fusion\x18\x0b \x01(\x08:\x05\x66\x61lse\x12\x1b\n\x0c\x65nable_addto\x18\x0c \x01(\x08:\x05\x66\x61lse\x12\'\n\x18\x61llow_cuda_graph_capture\x18\x0e \x01(\x08:\x05\x66\x61lse\x12\x1a\n\x0freduce_strategy\x18\x0f \x01(\x05:\x01\x30\x12!\n\x12\x66use_gemm_epilogue\x18\x10 \x01(\x08:\x05\x66\x61lse\x12\x1b\n\x13\x64\x65\x62ug_graphviz_path\x18\x11 \x01(\t\x12\x1e\n\x0f\x66used_attention\x18\x12 \x01(\x08:\x05\x66\x61lse\x12 \n\x11\x66used_feedforward\x18\x13 \x01(\x08:\x05\x66\x61lse\x12)\n\x1a\x66use_dot_product_attention\x18\x14 \x01(\x08:\x05\x66\x61lse\x12\x1b\n\x0c\x66use_resunit\x18\x15 \x01(\x08:\x05\x66\x61lse\"Q\n\x13GradientScaleConfig\x12\x1b\n\x0escale_strategy\x18\x01 \x01(\t:\x03\x61vg\x12\x1d\n\x0escale_gradient\x18\x02 \x01(\x08:\x05\x66\x61lse\"\xa3\x03\n\x0b\x41syncConfig\x12\x13\n\x07k_steps\x18\x01 \x01(\x05:\x02-1\x12\x1c\n\x11max_merge_var_num\x18\x02 \x01(\x05:\x01\x31\x12\x1b\n\x0fsend_queue_size\x18\x03 \x01(\x05:\x02\x31\x36\x12&\n\x17independent_recv_thread\x18\x04 \x01(\x08:\x05\x66\x61lse\x12(\n\x1dmin_send_grad_num_before_recv\x18\x05 \x01(\x05:\x01\x31\x12\x1b\n\x10thread_pool_size\x18\x06 \x01(\x05:\x01\x31\x12\x1a\n\x0fsend_wait_times\x18\x07 \x01(\x05:\x01\x31\x12&\n\x17runtime_split_send_recv\x18\x08 \x01(\x08:\x05\x66\x61lse\x12\x1c\n\x0elaunch_barrier\x18\t \x01(\x08:\x04true\x12&\n\x19heter_worker_device_guard\x18\n \x01(\t:\x03\x63pu\x12\x1a\n\x0elr_decay_steps\x18\x0b \x01(\x05:\x02\x31\x30\x12\x15\n\nuse_ps_gpu\x18\x0c \x01(\x05:\x01\x30\x12\x18\n\ruse_gpu_graph\x18\r \x01(\x05:\x01\x30\"\xc3\x01\n\x11TrainerDescConfig\x12\x18\n\x10\x64ump_fields_path\x18\x01 \x01(\t\x12\x13\n\x0b\x64ump_fields\x18\x02 \x03(\t\x12\x12\n\ndump_param\x18\x03 \x03(\t\x12\x16\n\x0estat_var_names\x18\x04 \x03(\t\x12\x0f\n\x07trainer\x18\x05 \x01(\t\x12\x15\n\rdevice_worker\x18\x06 \x01(\t\x12\x14\n\x0clocal_sparse\x18\x07 \x03(\t\x12\x15\n\rremote_sparse\x18\x08 \x03(\t\"\xae\x01\n\x0ePipelineConfig\x12\x1b\n\x10micro_batch_size\x18\x01 \x01(\x05:\x01\x31\x12\x1b\n\x10\x61\x63\x63umulate_steps\x18\x02 \x01(\x05:\x01\x31\x12\x1b\n\rschedule_mode\x18\x03 \x01(\t:\x04\x31\x46\x31\x42\x12\x1d\n\x0fp2p_cache_shape\x18\x04 \x01(\x08:\x04true\x12&\n\x18\x65nable_partial_send_recv\x18\x05 \x01(\x08:\x04true\"W\n\x14TensorParallelConfig\x12!\n\x16tensor_parallel_degree\x18\x01 \x01(\x05:\x01\x31\x12\x1c\n\x10tensor_init_seed\x18\x02 \x01(\x05:\x02-1\"\x8c\x01\n\tQatConfig\x12\"\n\x14\x63hannel_wise_abs_max\x18\x01 \x01(\x08:\x04true\x12\x16\n\x0bweight_bits\x18\x02 \x01(\x05:\x01\x38\x12\x1a\n\x0f\x61\x63tivation_bits\x18\x03 \x01(\x05:\x01\x38\x12\x19\n\x11not_quant_pattern\x18\x04 \x03(\t\x12\x0c\n\x04\x61lgo\x18\x05 \x01(\t\"\xb9\x03\n\x0eTableParameter\x12\x10\n\x08table_id\x18\x01 \x01(\x04\x12\x12\n\ntable_name\x18\x02 \x01(\t\x12\x13\n\x0btable_class\x18\x03 \x01(\t\x12\x17\n\tshard_num\x18\x04 \x01(\x04:\x04\x31\x30\x30\x30\x12%\n\x04type\x18\x05 \x01(\x0e\x32\x17.paddle.fleet.TableType\x12\x36\n\x08\x61\x63\x63\x65ssor\x18\x06 \x01(\x0b\x32$.paddle.fleet.TableAccessorParameter\x12\x1f\n\x10\x63ompress_in_save\x18\x07 \x01(\x08:\x05\x66\x61lse\x12\'\n\x19\x65nable_sparse_table_cache\x18\n \x01(\x08:\x04true\x12(\n\x17sparse_table_cache_rate\x18\x0b \x01(\x01:\x07\x30.00055\x12\'\n\x1bsparse_table_cache_file_num\x18\x0c \x01(\r:\x02\x31\x36\x12\x1c\n\renable_revert\x18\r \x01(\x08:\x05\x66\x61lse\x12\x1b\n\x10shard_merge_rate\x18\x0e \x01(\x02:\x01\x31\x12\x1c\n\ruse_gpu_graph\x18\x0f \x01(\x08:\x05\x66\x61lse\"\xac\x03\n\x16TableAccessorParameter\x12\x16\n\x0e\x61\x63\x63\x65ssor_class\x18\x01 \x01(\t\x12\x13\n\x07\x66\x65\x61_dim\x18\x04 \x01(\r:\x02\x31\x31\x12\x15\n\nembedx_dim\x18\x05 \x01(\r:\x01\x38\x12\x1c\n\x10\x65mbedx_threshold\x18\x06 \x01(\r:\x02\x31\x30\x12>\n\x12\x63tr_accessor_param\x18\x07 \x01(\x0b\x32\".paddle.fleet.CtrAccessorParameter\x12K\n\x19table_accessor_save_param\x18\x08 \x03(\x0b\x32(.paddle.fleet.TableAccessorSaveParameter\x12\x33\n\x0f\x65mbed_sgd_param\x18\n \x01(\x0b\x32\x1a.paddle.fleet.SGDParameter\x12\x34\n\x10\x65mbedx_sgd_param\x18\x0b \x01(\x0b\x32\x1a.paddle.fleet.SGDParameter\x12\x38\n\x0fgraph_sgd_param\x18\x0c \x01(\x0b\x32\x1f.paddle.fleet.GraphSGDParameter\"S\n\x11GraphSGDParameter\x12\x19\n\x0bnodeid_slot\x18\x01 \x01(\r:\x04\x39\x30\x30\x38\x12#\n\x15\x66\x65\x61ture_learning_rate\x18\x02 \x01(\x02:\x04\x30.05\"\xc8\x01\n\x0cSGDParameter\x12\x0c\n\x04name\x18\x01 \x01(\t\x12\x38\n\x05naive\x18\x02 \x01(\x0b\x32).paddle.fleet.SparseNaiveSGDRuleParameter\x12<\n\x07\x61\x64\x61grad\x18\x03 \x01(\x0b\x32+.paddle.fleet.SparseAdagradSGDRuleParameter\x12\x32\n\x04\x61\x64\x61m\x18\x04 \x01(\x0b\x32$.paddle.fleet.SparseAdamSGDParameter\"p\n\x1bSparseNaiveSGDRuleParameter\x12\x1b\n\rlearning_rate\x18\x01 \x01(\x01:\x04\x30.05\x12\x1d\n\rinitial_range\x18\x02 \x01(\x01:\x06\x30.0001\x12\x15\n\rweight_bounds\x18\x03 \x03(\x02\"\x8c\x01\n\x1dSparseAdagradSGDRuleParameter\x12\x1b\n\rlearning_rate\x18\x01 \x01(\x01:\x04\x30.05\x12\x18\n\rinitial_g2sum\x18\x02 \x01(\x01:\x01\x33\x12\x1d\n\rinitial_range\x18\x03 \x01(\x01:\x06\x30.0001\x12\x15\n\rweight_bounds\x18\x04 \x03(\x02\"\xc8\x01\n\x16SparseAdamSGDParameter\x12\x1c\n\rlearning_rate\x18\x01 \x01(\x01:\x05\x30.001\x12\x1d\n\rinitial_range\x18\x02 \x01(\x01:\x06\x30.0001\x12\x1d\n\x10\x62\x65ta1_decay_rate\x18\x03 \x01(\x01:\x03\x30.9\x12\x1f\n\x10\x62\x65ta2_decay_rate\x18\x04 \x01(\x01:\x05\x30.999\x12\x1a\n\x0b\x61\x64\x61_epsilon\x18\x05 \x01(\x01:\x05\x31\x65-08\x12\x15\n\rweight_bounds\x18\x06 \x03(\x02\"\x99\x03\n\x14\x43trAccessorParameter\x12\x19\n\x0cnonclk_coeff\x18\x01 \x01(\x02:\x03\x30.1\x12\x16\n\x0b\x63lick_coeff\x18\x02 \x01(\x02:\x01\x31\x12\x1b\n\x0e\x62\x61se_threshold\x18\x03 \x01(\x02:\x03\x31.5\x12\x1d\n\x0f\x64\x65lta_threshold\x18\x04 \x01(\x02:\x04\x30.25\x12\x1b\n\x0f\x64\x65lta_keep_days\x18\x05 \x01(\x02:\x02\x31\x36\x12#\n\x15show_click_decay_rate\x18\x06 \x01(\x02:\x04\x30.98\x12\x1d\n\x10\x64\x65lete_threshold\x18\x07 \x01(\x02:\x03\x30.8\x12$\n\x18\x64\x65lete_after_unseen_days\x18\x08 \x01(\x02:\x02\x33\x30\x12\"\n\x17ssd_unseenday_threshold\x18\t \x01(\x05:\x01\x31\x12\x18\n\nshow_scale\x18\n \x01(\x08:\x04true\x12\x17\n\tzero_init\x18\x0b \x01(\x08:\x04true\x12\x19\n\x11load_filter_slots\x18\x0c \x03(\x02\x12\x19\n\x11save_filter_slots\x18\r \x03(\x02\"S\n\x1aTableAccessorSaveParameter\x12\r\n\x05param\x18\x01 \x01(\r\x12\x11\n\tconverter\x18\x02 \x01(\t\x12\x13\n\x0b\x64\x65\x63onverter\x18\x03 \x01(\t\"R\n\x11\x46sClientParameter\x12\x0b\n\x03uri\x18\x01 \x01(\t\x12\x0c\n\x04user\x18\x02 \x01(\t\x12\x0e\n\x06passwd\x18\x03 \x01(\t\x12\x12\n\nhadoop_bin\x18\x04 \x01(\t\"\xe4\x12\n\x13\x44istributedStrategy\x12,\n\x04mode\x18\x01 \x01(\x0e\x32\x12.paddle.fleet.Mode:\nCOLLECTIVE\x12\x12\n\x03\x61mp\x18\x02 \x01(\x08:\x05\x66\x61lse\x12\x18\n\trecompute\x18\x03 \x01(\x08:\x05\x66\x61lse\x12\x17\n\x08localsgd\x18\x04 \x01(\x08:\x05\x66\x61lse\x12\x12\n\x03\x64gc\x18\x05 \x01(\x08:\x05\x66\x61lse\x12\x1d\n\x0egradient_merge\x18\x06 \x01(\x08:\x05\x66\x61lse\x12\x13\n\x04lars\x18\x07 \x01(\x08:\x05\x66\x61lse\x12\x13\n\x04lamb\x18\x08 \x01(\x08:\x05\x66\x61lse\x12\x17\n\x08pipeline\x18\t \x01(\x08:\x05\x66\x61lse\x12\x16\n\x07\x65lastic\x18\n \x01(\x08:\x05\x66\x61lse\x12\x13\n\x04\x61uto\x18\x0b \x01(\x08:\x05\x66\x61lse\x12\x14\n\x06\x61_sync\x18\x0c \x01(\x08:\x04true\x12!\n\x13sync_nccl_allreduce\x18\r \x01(\x08:\x04true\x12\x18\n\rnccl_comm_num\x18\x0e \x01(\x05:\x01\x31\x12)\n\x1ause_hierarchical_allreduce\x18\x0f \x01(\x08:\x05\x66\x61lse\x12.\n#hierarchical_allreduce_inter_nranks\x18\x10 \x01(\x05:\x01\x31\x12\x1e\n\x0fsync_batch_norm\x18\x11 \x01(\x08:\x05\x66\x61lse\x12!\n\x13\x66use_all_reduce_ops\x18\x12 \x01(\x08:\x04true\x12 \n\x14\x66use_grad_size_in_MB\x18\x13 \x01(\x05:\x02\x33\x32\x12$\n\x18\x66use_grad_size_in_TFLOPS\x18\x14 \x01(\x02:\x02\x35\x30\x12&\n\x17\x63udnn_exhaustive_search\x18\x15 \x01(\x08:\x05\x66\x61lse\x12&\n\x19\x63onv_workspace_size_limit\x18\x16 \x01(\x05:\x03\x35\x31\x32\x12\x31\n\"cudnn_batchnorm_spatial_persistent\x18\x17 \x01(\x08:\x05\x66\x61lse\x12 \n\x11\x61\x64\x61ptive_localsgd\x18\x18 \x01(\x08:\x05\x66\x61lse\x12\x1d\n\x0e\x66p16_allreduce\x18\x19 \x01(\x08:\x05\x66\x61lse\x12\x17\n\x08sharding\x18\x1a \x01(\x08:\x05\x66\x61lse\x12\"\n\x17last_comm_group_size_MB\x18\x1b \x01(\x02:\x01\x31\x12%\n\x16\x66ind_unused_parameters\x18\x1c \x01(\x08:\x05\x66\x61lse\x12\x1e\n\x0ftensor_parallel\x18\x1d \x01(\x08:\x05\x66\x61lse\x12(\n\x1awithout_graph_optimization\x18\x1e \x01(\x08:\x04true\x12 \n\x15\x66use_grad_size_in_num\x18\x1f \x01(\x05:\x01\x38\x12$\n\x15\x63\x61lc_comm_same_stream\x18  \x01(\x08:\x05\x66\x61lse\x12\x12\n\x03\x61sp\x18! \x01(\x08:\x05\x66\x61lse\x12\x1e\n\x0f\x66use_grad_merge\x18\" \x01(\x08:\x05\x66\x61lse\x12\x18\n\tsemi_auto\x18# \x01(\x08:\x05\x66\x61lse\x12\x19\n\nadam_d2sum\x18$ \x01(\x08:\x05\x66\x61lse\x12\x1a\n\x0b\x61uto_search\x18% \x01(\x08:\x05\x66\x61lse\x12\x1d\n\x0eheter_ccl_mode\x18& \x01(\x08:\x05\x66\x61lse\x12\x1c\n\ris_fl_ps_mode\x18\' \x01(\x08:\x05\x66\x61lse\x12\x1f\n\x10with_coordinator\x18( \x01(\x08:\x05\x66\x61lse\x12\x12\n\x03qat\x18) \x01(\x08:\x05\x66\x61lse\x12\x18\n\nsplit_data\x18* \x01(\x08:\x04true\x12\x38\n\x11recompute_configs\x18\x65 \x01(\x0b\x32\x1d.paddle.fleet.RecomputeConfig\x12,\n\x0b\x61mp_configs\x18\x66 \x01(\x0b\x32\x17.paddle.fleet.AMPConfig\x12\x36\n\x10localsgd_configs\x18g \x01(\x0b\x32\x1c.paddle.fleet.LocalSGDConfig\x12\x41\n\x16gradient_merge_configs\x18h \x01(\x0b\x32!.paddle.fleet.GradientMergeConfig\x12,\n\x0b\x64gc_configs\x18i \x01(\x0b\x32\x17.paddle.fleet.DGCConfig\x12\x36\n\x10pipeline_configs\x18j \x01(\x0b\x32\x1c.paddle.fleet.PipelineConfig\x12\x31\n\x0e\x61_sync_configs\x18k \x01(\x0b\x32\x19.paddle.fleet.AsyncConfig\x12.\n\x0clars_configs\x18l \x01(\x0b\x32\x18.paddle.fleet.LarsConfig\x12.\n\x0clamb_configs\x18m \x01(\x0b\x32\x18.paddle.fleet.LambConfig\x12G\n\x19\x61\x64\x61ptive_localsgd_configs\x18n \x01(\x0b\x32$.paddle.fleet.AdaptiveLocalSGDConfig\x12\x36\n\x10sharding_configs\x18o \x01(\x0b\x32\x1c.paddle.fleet.ShardingConfig\x12\x32\n\x0ehybrid_configs\x18p \x01(\x0b\x32\x1a.paddle.fleet.HybridConfig\x12\x43\n\x17tensor_parallel_configs\x18q \x01(\x0b\x32\".paddle.fleet.TensorParallelConfig\x12=\n\x14trainer_desc_configs\x18r \x01(\x0b\x32\x1f.paddle.fleet.TrainerDescConfig\x12:\n\x14\x64ownpour_table_param\x18s \x03(\x0b\x32\x1c.paddle.fleet.TableParameter\x12\x38\n\x0f\x66s_client_param\x18t \x01(\x0b\x32\x1f.paddle.fleet.FsClientParameter\x12,\n\x0bqat_configs\x18u \x01(\x0b\x32\x17.paddle.fleet.QatConfig\x12\x34\n\x0e\x62uild_strategy\x18\xc9\x01 \x01(\x0b\x32\x1b.paddle.fleet.BuildStrategy\x12\x42\n\x16gradient_scale_configs\x18\xcb\x01 \x01(\x0b\x32!.paddle.fleet.GradientScaleConfig\"\xfe\x01\n\x12\x44istributedJobInfo\x12\x12\n\nworker_num\x18\x01 \x01(\x05\x12\x12\n\nserver_num\x18\x02 \x01(\x05\x12\x12\n\nworker_ips\x18\x03 \x03(\t\x12\x18\n\x10server_endpoints\x18\x04 \x03(\t\x12\x16\n\x0eorigin_startup\x18\x05 \x01(\t\x12\x13\n\x0borigin_main\x18\x06 \x01(\t\x12\x18\n\x10\x64istributed_main\x18\x07 \x01(\t\x12\x16\n\x0eoptimizer_name\x18\x08 \x01(\t\x12\x33\n\x08strategy\x18\x65 \x01(\x0b\x32!.paddle.fleet.DistributedStrategy*7\n\x04Mode\x12\x0e\n\nCOLLECTIVE\x10\x01\x12\x06\n\x02PS\x10\x02\x12\x0c\n\x08PIPELINE\x10\x03\x12\t\n\x05HETER\x10\x04*4\n\tTableType\x12\x13\n\x0fPS_SPARSE_TABLE\x10\x00\x12\x12\n\x0ePS_DENSE_TABLE\x10\x01')

_builder.BuildMessageAndEnumDescriptors(DESCRIPTOR, globals())
_builder.BuildTopDescriptorsAndMessages(DESCRIPTOR, 'paddle.base.framework.distributed_strategy_pb2', globals())
if _descriptor._USE_C_DESCRIPTORS == False:

  DESCRIPTOR._options = None
  _MODE._serialized_start=10070
  _MODE._serialized_end=10125
  _TABLETYPE._serialized_start=10127
  _TABLETYPE._serialized_end=10179
  _REFINEDOPSPATTERN._serialized_start=67
  _REFINEDOPSPATTERN._serialized_end=154
  _RECOMPUTECONFIG._serialized_start=157
  _RECOMPUTECONFIG._serialized_end=345
  _SHARDINGCONFIG._serialized_start=348
  _SHARDINGCONFIG._serialized_end=862
  _MPCONFIG._serialized_start=865
  _MPCONFIG._serialized_end=1213
  _PPCONFIG._serialized_start=1216
  _PPCONFIG._serialized_end=1721
  _DYGRAPHSHARDINGCONFIG._serialized_start=1724
  _DYGRAPHSHARDINGCONFIG._serialized_end=2071
  _HYBRIDCONFIG._serialized_start=2074
  _HYBRIDCONFIG._serialized_end=2482
  _AMPCONFIG._serialized_start=2485
  _AMPCONFIG._serialized_end=2898
  _LOCALSGDCONFIG._serialized_start=2900
  _LOCALSGDCONFIG._serialized_end=2959
  _ADAPTIVELOCALSGDCONFIG._serialized_start=2961
  _ADAPTIVELOCALSGDCONFIG._serialized_end=3033
  _GRADIENTMERGECONFIG._serialized_start=3035
  _GRADIENTMERGECONFIG._serialized_end=3095
  _DGCCONFIG._serialized_start=3097
  _DGCCONFIG._serialized_end=3180
  _LARSCONFIG._serialized_start=3183
  _LARSCONFIG._serialized_end=3312
  _LAMBCONFIG._serialized_start=3314
  _LAMBCONFIG._serialized_end=3394
  _BUILDSTRATEGY._serialized_start=3397
  _BUILDSTRATEGY._serialized_end=4088
  _GRADIENTSCALECONFIG._serialized_start=4090
  _GRADIENTSCALECONFIG._serialized_end=4171
  _ASYNCCONFIG._serialized_start=4174
  _ASYNCCONFIG._serialized_end=4593
  _TRAINERDESCCONFIG._serialized_start=4596
  _TRAINERDESCCONFIG._serialized_end=4791
  _PIPELINECONFIG._serialized_start=4794
  _PIPELINECONFIG._serialized_end=4968
  _TENSORPARALLELCONFIG._serialized_start=4970
  _TENSORPARALLELCONFIG._serialized_end=5057
  _QATCONFIG._serialized_start=5060
  _QATCONFIG._serialized_end=5200
  _TABLEPARAMETER._serialized_start=5203
  _TABLEPARAMETER._serialized_end=5644
  _TABLEACCESSORPARAMETER._serialized_start=5647
  _TABLEACCESSORPARAMETER._serialized_end=6075
  _GRAPHSGDPARAMETER._serialized_start=6077
  _GRAPHSGDPARAMETER._serialized_end=6160
  _SGDPARAMETER._serialized_start=6163
  _SGDPARAMETER._serialized_end=6363
  _SPARSENAIVESGDRULEPARAMETER._serialized_start=6365
  _SPARSENAIVESGDRULEPARAMETER._serialized_end=6477
  _SPARSEADAGRADSGDRULEPARAMETER._serialized_start=6480
  _SPARSEADAGRADSGDRULEPARAMETER._serialized_end=6620
  _SPARSEADAMSGDPARAMETER._serialized_start=6623
  _SPARSEADAMSGDPARAMETER._serialized_end=6823
  _CTRACCESSORPARAMETER._serialized_start=6826
  _CTRACCESSORPARAMETER._serialized_end=7235
  _TABLEACCESSORSAVEPARAMETER._serialized_start=7237
  _TABLEACCESSORSAVEPARAMETER._serialized_end=7320
  _FSCLIENTPARAMETER._serialized_start=7322
  _FSCLIENTPARAMETER._serialized_end=7404
  _DISTRIBUTEDSTRATEGY._serialized_start=7407
  _DISTRIBUTEDSTRATEGY._serialized_end=9811
  _DISTRIBUTEDJOBINFO._serialized_start=9814
  _DISTRIBUTEDJOBINFO._serialized_end=10068
# @@protoc_insertion_point(module_scope)
